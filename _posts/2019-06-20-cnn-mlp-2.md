---
layout: post
title:  "Sparse multilayer perceptrons: converting CNNs to MLPs - Part 2"
date:   2019-06-20 22:00:00 +0200
categories: machinelearning
---
This is the second part of my series about converting CNNs to MLPs, the first part can be read [here](https://aul12.me/machinelearning/2019/06/10/cnn-mlp-1.html).
In this post i will first study the time and space complexity of the converted CNN and then verify these results using actual CNNs converted to OpenCV-MLP models.

If you are only interested in the results, scroll to the bottom of the page. 
If you are interested in the more theoretical results i advise you to read the first part first, i will refer to variables defined in the first part.

## Complexity of CNNs and MLPs
### Time complexity
### Space complexity
As in part 1 i will simplify this derivation to grayscale images. The kernel of the discrete convolution is of size $${(2 \cdot k + 1)}^2$$, the bias of size $$1$$,
this means the space complexity of a single filter in a single layer is 

$${(2 \cdot k + 1)}^2 + 1 \in \mathcal{O}(k^2)$$

which espacially means the space complexity is independent of the input size. 
For a MLP the size of the weight matrix is $${(n \cdot m)}^2$$, the bias of size $$n \cdot m$$. 
This means the space complexity of a single filter in a single layer is:

$${(n \cdot m)}^2 + n \cdot m \in \mathcal{O}(n^2 \cdot m^2)$$

For both architectures the space complexity scales linearly with the number as filters as well as the number of layers. One thing to keep in mind, is that the conversion to a MLP requires the adaption of pooling layers to normal fully connected layers. While a pooling layer requires no learned parameter, the converted layer is another layer with parameters of complexity $$\mathcal{O}(n^2 \cdot m^2)$$
